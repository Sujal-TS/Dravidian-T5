{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: sacremoses in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: evaluate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk pandas sacremoses transformers sentencepiece datasets torch evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (2.2.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.8.61)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"aligned_merged_sentences.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "                                             Kannada  \\\n",
      "0                             ಚುನಾವಣೆ ನಡೆಯುವುದಿಲ್ಲ .   \n",
      "1  ಮಾತು ಮುಂದುವರೆಸಿದ ಮಾಜಿ ಸಚಿವ ವಿ. ಶ್ರೀನಿವಾಸ್ ಪ್ರಸ...   \n",
      "2  ತನ್ನ ಪರೀಕ್ಷಾ ಸಾಮರ್ಥ್ಯವನ್ನು ಹೆಚ್ಚಿಸಿಕೊಳ್ಳಲು, ವೈ...   \n",
      "3                 ಹತ್ಯೆಗೆ ಸ್ಪಷ್ಟ ಕಾರಣ ತಿಳಿದುಬಂದಿಲ್ಲ.   \n",
      "4     ಮತ್ತು ಇದನ್ನು ಬೆಳೆಸುವ ವಿಧಾನವೂ ಅಷ್ಟೇನು ಕಷ್ಟವಲ್ಲ.   \n",
      "5  ಅಪೊಸ್ತಲ ಪೌಲನು ನಂಬಿಗಸ್ತಿಕೆಯಿಂದ ಸಾರುವ ಕಾರ್ಯದಲ್ಲಿ...   \n",
      "6  18 ವರ್ಷದೊಳಗಿನ ಪತ್ನಿ ಜೊತೆ ಸಂಬಂಧ ಬೆಳೆಸಿದ್ರೆ ಅದನ್...   \n",
      "7  ಮುಂಬೈ: ಪುಲ್ವಾಮಾ ದಾಳಿ ಬಳಿಕ ಭಾರತದಲ್ಲಿ ಪಾಕಿಸ್ತಾನ ...   \n",
      "8              ಹೀಗೆ ಹಲವಾರು ಕಾರ್ಯಕ್ರಮಗಳನ್ನು ರೂಪಿಸಿದೆ.   \n",
      "9                         ನೋಕಿಯಾ ಲುಮಿಯಾ 800 ವಿಶೇಷತೆ:   \n",
      "\n",
      "                                             English  \n",
      "0                              There is no election.  \n",
      "1  Former Minister V. Sreenivasa Prasad will pres...  \n",
      "2  To boost its testing capacity, Council of Scie...  \n",
      "3        No specific reason for the murder is known.  \n",
      "4      And it's not too difficult to do that either.  \n",
      "5  The apostle Paul was physically beaten more th...  \n",
      "6  Having sexual relationship with a girl below t...  \n",
      "7  Mumbai: After Pulwama terror attack in Jammu a...  \n",
      "8              A number of events have been planned.  \n",
      "9                                    Nokia lumia 800  \n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "similarity_model = SentenceTransformer('l3cube-pune/indic-sentence-similarity-sbert')\n",
    " \n",
    "assert \"Kannada\" in df.columns and \"English\" in df.columns, \"Missing Kannada or English columns in dataset!\"\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    embeddings1 = similarity_model.encode([text1])\n",
    "    embeddings2 = similarity_model.encode([text2])\n",
    "    similarity_score = 1 - cosine(embeddings1[0], embeddings2[0]) \n",
    "    return similarity_score\n",
    "\n",
    "valid_indices = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    similarity = calculate_cosine_similarity(row['Kannada'], row['English'])\n",
    "    \n",
    "    print(f\"Comparing Kannada: '{row['Kannada']}'\")\n",
    "    print(f\"With English: '{row['English']}'\")\n",
    "    print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "    \n",
    "    if similarity > 0.7:\n",
    "        print(f\"Sentence pair kept (cosine similarity > 0.7)\")\n",
    "        valid_indices.append(idx)\n",
    "    else:\n",
    "        print(f\"Sentence pair removed (cosine similarity <= 0.7)\")\n",
    "    \n",
    "    print('-' * 50)  \n",
    "\n",
    "df_filtered = df.loc[valid_indices]\n",
    "\n",
    "df_filtered.to_csv(\"filtered_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Filtered Dataset (First 10 rows):\")\n",
    "print(df_filtered.head(10))\n",
    "\n",
    "print(\"Filtering complete! Saved as 'filtered_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "similarity_model = SentenceTransformer('l3cube-pune/indic-sentence-similarity-sbert')\n",
    "\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating to tam_Taml: 100%|██████████| 73/73 [00:13<00:00,  5.59it/s]\n",
      "Translating to tel_Telu: 100%|██████████| 73/73 [00:12<00:00,  5.68it/s]\n",
      "Translating to mal_Mlym: 100%|██████████| 73/73 [00:12<00:00,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Kannada  \\\n",
      "0                              ಚುನಾವಣೆ ನಡೆಯುವುದಿಲ್ಲ .   \n",
      "2   ತನ್ನ ಪರೀಕ್ಷಾ ಸಾಮರ್ಥ್ಯವನ್ನು ಹೆಚ್ಚಿಸಿಕೊಳ್ಳಲು, ವೈ...   \n",
      "3                  ಹತ್ಯೆಗೆ ಸ್ಪಷ್ಟ ಕಾರಣ ತಿಳಿದುಬಂದಿಲ್ಲ.   \n",
      "4      ಮತ್ತು ಇದನ್ನು ಬೆಳೆಸುವ ವಿಧಾನವೂ ಅಷ್ಟೇನು ಕಷ್ಟವಲ್ಲ.   \n",
      "6   18 ವರ್ಷದೊಳಗಿನ ಪತ್ನಿ ಜೊತೆ ಸಂಬಂಧ ಬೆಳೆಸಿದ್ರೆ ಅದನ್...   \n",
      "8               ಹೀಗೆ ಹಲವಾರು ಕಾರ್ಯಕ್ರಮಗಳನ್ನು ರೂಪಿಸಿದೆ.   \n",
      "9                          ನೋಕಿಯಾ ಲುಮಿಯಾ 800 ವಿಶೇಷತೆ:   \n",
      "10  ಬದಲಾಗಿ ಯೋನಾತಾನನು ದಾವೀದನ ಆಪ್ತಮಿತ್ರನೂ ಸಹಾಯಕನೂ ಆಗ...   \n",
      "11  ‘ ಒಂದು ಹಾಡಿನಲ್ಲಿ ದೇವರನ್ನು ಮಹಿಮೆಪಡಿಸುವುದು ಯಾವ ಅ...   \n",
      "12                              ಅಮ್ಮು ನಿನಗೆ ನೆನಪಿದೆಯ?   \n",
      "\n",
      "                                              English  \\\n",
      "0                               There is no election.   \n",
      "2   To boost its testing capacity, Council of Scie...   \n",
      "3         No specific reason for the murder is known.   \n",
      "4       And it's not too difficult to do that either.   \n",
      "6   Having sexual relationship with a girl below t...   \n",
      "8               A number of events have been planned.   \n",
      "9                                     Nokia lumia 800   \n",
      "10  On the contrary, Jonathan was Davids close fri...   \n",
      "11  Taken aback, the teacher wondered, What could ...   \n",
      "12                             You remember that mom?   \n",
      "\n",
      "                                                Tamil  \\\n",
      "0                               Δεν υπάρχουν εκλογές.   \n",
      "2   ਆਪਣੀ ਟੈਸਟਿੰਗ ਸਮਰੱਥਾ ਨੂੰ ਵਧਾਉਣ ਲਈ, ਕੌਂਸਲ ਆਫ ਸਾਇ...   \n",
      "3              Cinayetin kesin bir nedeni bilinmiyor.   \n",
      "4                   Y tampoco es muy difícil hacerlo.   \n",
      "6   Go tlhakanela dikobo le mosetsana yo o ka fa t...   \n",
      "8             Plusieurs événements ont été planifiés.   \n",
      "9                                     Nokia lumia 800   \n",
      "10  Ngũrani na ũguo, Jonathani aarĩ mũrata wa haku...   \n",
      "11  Dispela tisa i kirap nogut na em i askim em, W...   \n",
      "12                         Îţi aminteşti de mama aia?   \n",
      "\n",
      "                                               Telugu  \\\n",
      "0                               Δεν υπάρχουν εκλογές.   \n",
      "2   ਆਪਣੀ ਟੈਸਟਿੰਗ ਸਮਰੱਥਾ ਨੂੰ ਵਧਾਉਣ ਲਈ, ਕੌਂਸਲ ਆਫ ਸਾਇ...   \n",
      "3              Cinayetin kesin bir nedeni bilinmiyor.   \n",
      "4                   Y tampoco es muy difícil hacerlo.   \n",
      "6   Go tlhakanela dikobo le mosetsana yo o ka fa t...   \n",
      "8             Plusieurs événements ont été planifiés.   \n",
      "9                                     Nokia lumia 800   \n",
      "10  Ngũrani na ũguo, Jonathani aarĩ mũrata wa haku...   \n",
      "11  Dispela tisa i kirap nogut na em i askim em, W...   \n",
      "12                         Îţi aminteşti de mama aia?   \n",
      "\n",
      "                                            Malayalam  \n",
      "0                               Δεν υπάρχουν εκλογές.  \n",
      "2   ਆਪਣੀ ਟੈਸਟਿੰਗ ਸਮਰੱਥਾ ਨੂੰ ਵਧਾਉਣ ਲਈ, ਕੌਂਸਲ ਆਫ ਸਾਇ...  \n",
      "3              Cinayetin kesin bir nedeni bilinmiyor.  \n",
      "4                   Y tampoco es muy difícil hacerlo.  \n",
      "6   Go tlhakanela dikobo le mosetsana yo o ka fa t...  \n",
      "8             Plusieurs événements ont été planifiés.  \n",
      "9                                     Nokia lumia 800  \n",
      "10  Ngũrani na ũguo, Jonathani aarĩ mũrata wa haku...  \n",
      "11  Dispela tisa i kirap nogut na em i askim em, W...  \n",
      "12                         Îţi aminteşti de mama aia?  \n",
      "Translation complete! Saved as 'translated_next50k_dataset.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Kannada', 'Tamil', 'Telugu', 'Malayalam'],\n",
      "        num_rows: 90\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Kannada', 'Tamil', 'Telugu', 'Malayalam'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "df = pd.read_csv(\"final_dataset.csv\")  \n",
    "df=df.head(100)\n",
    "df = df[[\"Kannada\", \"Tamil\", \"Telugu\", \"Malayalam\"]].dropna()\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = hf_dataset.train_test_split(test_size=0.1)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 512)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: matplotlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch sentencepiece matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.4.0)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-5.3.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading lxml-5.3.1-cp310-cp310-manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m152.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 lxml-5.3.1 portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets sentencepiece torch accelerate sacrebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: sentencepiece in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: evaluate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: sacrebleu in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.61)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: portalocker in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sacrebleu) (5.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch sentencepiece accelerate evaluate sacrebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb51759e1494438b92395069e14798a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771acf9a826a45dab869c1e52978db20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_41432/48624153.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56250' max='56250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56250/56250 5:01:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.607400</td>\n",
       "      <td>0.446499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.605200</td>\n",
       "      <td>0.407977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.535200</td>\n",
       "      <td>0.394828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.461200</td>\n",
       "      <td>0.388340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>0.386353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=56250, training_loss=1.2188820366753472, metrics={'train_runtime': 18111.8458, 'train_samples_per_second': 12.423, 'train_steps_per_second': 3.106, 'total_flos': 2.9742170112e+16, 'train_loss': 1.2188820366753472, 'epoch': 5.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, MT5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "\n",
    "data_pairs = []\n",
    "for _, row in df.iterrows():\n",
    "    for src_lang, tgt_lang in [(\"Kannada\", \"Tamil\"), (\"Kannada\", \"Telugu\"), (\"Kannada\", \"Malayalam\")]:\n",
    "        data_pairs.append({\n",
    "            \"input\": f\"<extra_id_0> {src_lang} → {tgt_lang}: {row[src_lang]}\",\n",
    "            \"target\": row[tgt_lang]\n",
    "        })\n",
    "        data_pairs.append({\n",
    "            \"input\": f\"<extra_id_0> {tgt_lang} → {src_lang}: {row[tgt_lang]}\",\n",
    "            \"target\": row[src_lang]\n",
    "        })\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data_pairs))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"input\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"target\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = dataset.shuffle(seed=42).select([i for i in range(45000)]) \n",
    "val_dataset = dataset.select([i for i in range(45000, 48618)])  \n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             \n",
    "    evaluation_strategy=\"epoch\",       \n",
    "    learning_rate=2e-5,               \n",
    "    per_device_train_batch_size=4,      \n",
    "    per_device_eval_batch_size=4,      \n",
    "    num_train_epochs=5,               \n",
    "    weight_decay=0.01,                 \n",
    "    logging_dir=\"./logs\",              \n",
    "    logging_steps=10,                   \n",
    "    save_strategy=\"epoch\",          \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,        \n",
    "    eval_dataset=val_dataset,           \n",
    "    tokenizer=tokenizer,                \n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to Sujal-mT5\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "\n",
    "checkpoint_path = \"results/checkpoint-56250\"\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "\n",
    "save_path = \"Sujal-mT5\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "\n",
    "model_path = \"Sujal-mT5\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(input_text, target_language):\n",
    "   \n",
    "    input_text = f\"<extra_id_0> Kannada → {target_language}: {input_text}\"\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    \n",
    "    output_ids = model.generate(inputs[\"input_ids\"], num_beams=10, max_length=128, early_stopping=True)\n",
    "    \n",
    "  \n",
    "    translated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to Tamil: நான் பள்ளியில் இருந்தேன்.\n",
      "Translated to Telugu: నేను ಶಾಲೆಗೆ వచ్చాను\n",
      "Translated to Malayalam: ഞാന് പഠിച്ചു.\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"ನಾನು ಶಾಲೆಗೆ ಹೋಗಿದ್ದೇನೆ\"  \n",
    "\n",
    " \n",
    "translated_sentence_tamil = translate_text(input_sentence, target_language=\"Tamil\")\n",
    "print(\"Translated to Tamil:\", translated_sentence_tamil)\n",
    "\n",
    "\n",
    "translated_sentence_telugu = translate_text(input_sentence, target_language=\"Telugu\")\n",
    "print(\"Translated to Telugu:\", translated_sentence_telugu)\n",
    "\n",
    "translated_sentence_malayalam = translate_text(input_sentence, target_language=\"Malayalam\")\n",
    "print(\"Translated to Malayalam:\", translated_sentence_malayalam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to kannada: ನಾನು ಕನ್ನಡದ ಬಗ್ಗೆ ಮಾತನಾಡಿದ್ದೇನೆ\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"నేను తెలుగు నేర్చుకుంటున్నాను\"  \n",
    "\n",
    "translated_sentence_telugu = translate_text(input_sentence, target_language=\"Kannada\")\n",
    "print(\"Translated to kannada:\", translated_sentence_telugu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to kannada: ನಾನು ಶಾಲೆಗೆ ಬಂದಿದ್ದೇನೆ\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"నేను ಶಾಲೆಗೆ వచ్చాను\"  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "translated_sentence_telugu = translate_text(input_sentence, target_language=\"Kannada\")\n",
    "print(\"Translated to kannada:\", translated_sentence_telugu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to Tamil: அவருக்கு நன்றிகள்.\n",
      "Translated to Telugu: అతను నాకు కృతజ్ఞతలు చెప్పారు.\n",
      "Translated to Malayalam: അദ്ദേഹം എനിക്ക് നന്ദി പറഞ്ഞു.\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"ಅವಳು ನನಗೆ ಧನ್ಯವಾದಗಳು ಹೇಳಿದಳು.\"  \n",
    "\n",
    "translated_sentence_tamil = translate_text(input_sentence, target_language=\"Tamil\")\n",
    "print(\"Translated to Tamil:\", translated_sentence_tamil)\n",
    "\n",
    "\n",
    "translated_sentence_telugu = translate_text(input_sentence, target_language=\"Telugu\")\n",
    "print(\"Translated to Telugu:\", translated_sentence_telugu)\n",
    "\n",
    "\n",
    "translated_sentence_malayalam = translate_text(input_sentence, target_language=\"Malayalam\")\n",
    "print(\"Translated to Malayalam:\", translated_sentence_malayalam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated to Tamil: நான் உங்கள் பயணத்தில் ಹಲವು அழகான காட்சிகளை பார்த்தேன்.\n",
      "Translated to Telugu: నేను మీరు ప్రయాణించినప్పుడు చాలా అందమైన ప్రదేశాలు చూసాను.\n",
      "Translated to Malayalam: എന്റെ കൂടെ യാത്ര ചെയ്യുമ്പോൾ ഞാന് വളരെ സന്തോഷിച്ചു.\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"ನಾನು ನಿಮ್ಮೊಂದಿಗೆ ಪ್ರಯಾಣ ಮಾಡುತ್ತಿರುವಾಗ ನಾನು ಹಲವು ಸುಂದರವಾದ ದೃಶ್ಯಗಳನ್ನು ನೋಡಿದೆನು.\" \n",
    "\n",
    "\n",
    "translated_sentence_tamil = translate_text(input_sentence, target_language=\"Tamil\")\n",
    "print(\"Translated to Tamil:\", translated_sentence_tamil)\n",
    "\n",
    "\n",
    "translated_sentence_telugu = translate _text(input_sentence, target_language=\"Telugu\")\n",
    "print(\"Translated to Telugu:\", translated_sentence_telugu)\n",
    "\n",
    "\n",
    "translated_sentence_malayalam = translate_text(input_sentence, target_language=\"Malayalam\")\n",
    "print(\"Translated to Malayalam:\", translated_sentence_malayalam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Load your model from the local directory\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Sujal-mT5\")\n",
    "\n",
    "# Save its weights to a .pth file\n",
    "torch.save(model.state_dict(), \"sujal_mt5_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping from language name to NLLB-200 language code\n",
    "LANG_CODE_MAP = {\n",
    "    \"Kannada\": \"kan_Knda\",\n",
    "    \"Telugu\": \"tel_Telu\",\n",
    "    \"Tamil\": \"tam_Taml\",\n",
    "    \"Malayalam\": \"mal_Mlym\"\n",
    "}\n",
    "\n",
    "def translate_from_input_format(input_line):\n",
    "    \"\"\"\n",
    "    Translates a line in the format: <SourceLanguage> <Sentence> <TargetLanguage>\n",
    "    \n",
    "    Args:\n",
    "        input_line (str): Full input string\n",
    "    \n",
    "    Returns:\n",
    "        str: Translated sentence\n",
    "    \"\"\"\n",
    "    words = input_line.strip().split()\n",
    "    src_lang = words[0]\n",
    "    tgt_lang = words[-1]\n",
    "    sentence = \" \".join(words[1:-1])\n",
    "    \n",
    "    src_code = LANG_CODE_MAP.get(src_lang)\n",
    "    tgt_code = LANG_CODE_MAP.get(tgt_lang)\n",
    "\n",
    "    if not src_code or not tgt_code:\n",
    "        return f\"Unsupported language(s): {src_lang}, {tgt_lang}\"\n",
    "    \n",
    "    tokenizer.src_lang = src_code\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_code),\n",
    "        max_length=50\n",
    "    )\n",
    "\n",
    "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: నేను ఒక ఆట ఆడటానికి వెళ్ళాను.\n"
     ]
    }
   ],
   "source": [
    "input_line = \"Kannada ನಾನು ಆಟ ಆಡಲು ಹೋದೆ. Telugu\"\n",
    "translated = translate_from_input_format(input_line)\n",
    "print(\"Translated Sentence:\", translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Sentence: ನಾನು ಆಟವಾಡಲು ಹೋಗಿದ್ದೆ.\n"
     ]
    }
   ],
   "source": [
    "input_line = \"Telugu నేను ఒక ఆట ఆడటానికి వెళ్ళాను. Kannada\"\n",
    "translated = translate_from_input_format(input_line)\n",
    "print(\"Translated Sentence:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 09:31:47.704579: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 09:31:48.121733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747301508.275247    1794 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747301508.330125    1794 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747301508.677053    1794 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747301508.677324    1794 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747301508.677330    1794 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747301508.677333    1794 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 09:31:48.720118: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 512)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load your trained model/tokenizer\n",
    "model_path = \"Sujal-mT5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['Kannada', 'Tamil', 'Telugu', 'Malayalam']\n",
    "lang_code_map = {\n",
    "    'Kannada': 'Kannada',\n",
    "    'Tamil': 'Tamil',\n",
    "    'Telugu': 'Telugu',\n",
    "    'Malayalam': 'Malayalam'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c957ee00660f47079df544c7bf79988a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056dc61164444de196115b6b1c338387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514e9e9d4dd24d13a1abbed83e3a0a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b5b39b38d94e5cbabe78de9c375247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d92310e26524752a13e3901dce735fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b59b65bf0e0417d9a3a782c11d54dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/950M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e26c16576ca44f8a9221e5bbed28a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26abafb5ae404c3ba912a050efebafe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1429354d7d4d9d920765e2cc9e4468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/950M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d09397e0244f908d1c59e90a70481d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be59c670a474248b7a809e0680699cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339cd650861c4ff4a9ba3918907a40bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "langs = ['Kannada', 'Tamil', 'Telugu', 'Malayalam']\n",
    "lang_code_map = {\n",
    "    'Kannada': 'Kannada',\n",
    "    'Tamil': 'Tamil',\n",
    "    'Telugu': 'Telugu',\n",
    "    'Malayalam': 'Malayalam'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(texts, src_lang, tgt_lang):\n",
    "    prefix = f\"translate {lang_code_map[src_lang]} to {lang_code_map[tgt_lang]}: \"\n",
    "    inputs = tokenizer([prefix + text for text in texts], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "def sentence_bleu_score(reference, candidate):\n",
    "    return sentence_bleu(\n",
    "        [reference.split()],\n",
    "        candidate.split(),\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),\n",
    "        smoothing_function=smoother\n",
    "    )\n",
    "\n",
    "def sentence_similarity(ref, cand):\n",
    "    emb_ref = sim_model.encode(ref, convert_to_tensor=True)\n",
    "    emb_cand = sim_model.encode(cand, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(emb_ref, emb_cand).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Kannada -> Kannada (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Kannada -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Tamil: 100%|██████████| 100/100 [00:02<00:00, 38.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 5.52%, Avg Similarity: 0.4903\n",
      "\n",
      "Translating Kannada -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Telugu: 100%|██████████| 100/100 [00:02<00:00, 41.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 6.75%, Avg Similarity: 0.4484\n",
      "\n",
      "Translating Kannada -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Malayalam: 100%|██████████| 100/100 [00:02<00:00, 40.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 5.39%, Avg Similarity: 0.5168\n",
      "\n",
      "Translating Tamil -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Kannada: 100%|██████████| 100/100 [00:01<00:00, 53.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 4.94%, Avg Similarity: 0.5013\n",
      "Skipping Tamil -> Tamil (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Tamil -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Telugu: 100%|██████████| 100/100 [00:01<00:00, 51.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 5.98%, Avg Similarity: 0.5146\n",
      "\n",
      "Translating Tamil -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Malayalam: 100%|██████████| 100/100 [00:01<00:00, 51.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 3.22%, Avg Similarity: 0.5163\n",
      "\n",
      "Translating Telugu -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Kannada: 100%|██████████| 100/100 [00:01<00:00, 51.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 5.01%, Avg Similarity: 0.4613\n",
      "\n",
      "Translating Telugu -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Tamil: 100%|██████████| 100/100 [00:01<00:00, 51.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 5.08%, Avg Similarity: 0.4905\n",
      "Skipping Telugu -> Telugu (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Telugu -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Malayalam: 100%|██████████| 100/100 [00:01<00:00, 53.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 3.25%, Avg Similarity: 0.5143\n",
      "\n",
      "Translating Malayalam -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Kannada: 100%|██████████| 100/100 [00:01<00:00, 52.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 5.07%, Avg Similarity: 0.4510\n",
      "\n",
      "Translating Malayalam -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Tamil: 100%|██████████| 100/100 [00:01<00:00, 53.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 4.96%, Avg Similarity: 0.4537\n",
      "\n",
      "Translating Malayalam -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Telugu: 100%|██████████| 100/100 [00:01<00:00, 53.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [100/100] Avg BLEU: 2.69%, Avg Similarity: 0.4958\n",
      "Skipping Malayalam -> Malayalam (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "BLEU Scores Matrix (%):\n",
      "           Kannada   Tamil  Telugu  Malayalam\n",
      "Kannada     100.00    5.52    6.75       5.39\n",
      "Tamil         4.94  100.00    5.98       3.22\n",
      "Telugu        5.01    5.08  100.00       3.25\n",
      "Malayalam     5.07    4.96    2.69     100.00\n",
      "\n",
      "Similarity Scores Matrix:\n",
      "           Kannada   Tamil  Telugu  Malayalam\n",
      "Kannada     1.0000  0.4903  0.4484     0.5168\n",
      "Tamil       0.5013  1.0000  0.5146     0.5163\n",
      "Telugu      0.4613  0.4905  1.0000     0.5143\n",
      "Malayalam   0.4510  0.4537  0.4958     1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm  # for progress bars\n",
    "\n",
    "bleu_df = pd.DataFrame(index=langs, columns=langs, dtype=float)\n",
    "sim_df = pd.DataFrame(index=langs, columns=langs, dtype=float)\n",
    "\n",
    "# Compute BLEU and similarity for each language pair\n",
    "for src_lang in langs:\n",
    "    src_texts = df[src_lang].tolist()\n",
    "    for tgt_lang in langs:\n",
    "        tgt_texts = df[tgt_lang].tolist()\n",
    "        if src_lang == tgt_lang:\n",
    "            bleu_df.loc[src_lang, tgt_lang] = 100.0\n",
    "            sim_df.loc[src_lang, tgt_lang] = 1.0\n",
    "            print(f\"Skipping {src_lang} -> {tgt_lang} (same language), BLEU=100, Similarity=1.0\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTranslating {src_lang} -> {tgt_lang} ...\")\n",
    "        translations = translate_batch(src_texts, src_lang, tgt_lang)\n",
    "\n",
    "        bleu_scores = []\n",
    "        sim_scores = []\n",
    "        # Use tqdm for sentence-level progress bar\n",
    "        for i, (ref, pred) in enumerate(tqdm(zip(tgt_texts, translations), total=len(tgt_texts), desc=f\"{src_lang}->{tgt_lang}\")):\n",
    "            try:\n",
    "                bleu_val = sentence_bleu_score(ref, pred)\n",
    "            except Exception as e:\n",
    "                bleu_val = 0.0\n",
    "            sim_val = sentence_similarity(ref, pred)\n",
    "            bleu_scores.append(bleu_val)\n",
    "            sim_scores.append(sim_val)\n",
    "\n",
    "            # Print progress every 100 sentences (optional)\n",
    "            if (i + 1) % 100 == 0 or i == len(tgt_texts) - 1:\n",
    "                avg_bleu = np.mean(bleu_scores) * 100\n",
    "                avg_sim = np.mean(sim_scores)\n",
    "                print(f\"  [{i+1}/{len(tgt_texts)}] Avg BLEU: {avg_bleu:.2f}%, Avg Similarity: {avg_sim:.4f}\")\n",
    "\n",
    "        avg_bleu = np.mean(bleu_scores) * 100\n",
    "        avg_sim = np.mean(sim_scores)\n",
    "\n",
    "        bleu_df.loc[src_lang, tgt_lang] = avg_bleu\n",
    "        sim_df.loc[src_lang, tgt_lang] = avg_sim\n",
    "\n",
    "print(\"\\nBLEU Scores Matrix (%):\")\n",
    "print(bleu_df.round(2))\n",
    "\n",
    "print(\"\\nSimilarity Scores Matrix:\")\n",
    "print(sim_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Kannada -> Kannada (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Kannada -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Tamil: 100%|██████████| 2/2 [00:00<00:00, 51.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Kannada -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Telugu: 100%|██████████| 2/2 [00:00<00:00, 52.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Kannada -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Malayalam: 100%|██████████| 2/2 [00:00<00:00, 51.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Tamil -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Kannada: 100%|██████████| 2/2 [00:00<00:00, 51.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Tamil -> Tamil (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Tamil -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Telugu: 100%|██████████| 2/2 [00:00<00:00, 53.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Tamil -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Malayalam: 100%|██████████| 2/2 [00:00<00:00, 52.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Telugu -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Kannada: 100%|██████████| 2/2 [00:00<00:00, 51.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Telugu -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Tamil: 100%|██████████| 2/2 [00:00<00:00, 51.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Telugu -> Telugu (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Telugu -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Malayalam: 100%|██████████| 2/2 [00:00<00:00, 51.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Malayalam -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Kannada: 100%|██████████| 2/2 [00:00<00:00, 52.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Malayalam -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Tamil: 100%|██████████| 2/2 [00:00<00:00, 52.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Malayalam -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Telugu: 100%|██████████| 2/2 [00:00<00:00, 52.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Malayalam -> Malayalam (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "BLEU Scores Matrix (%):\n",
      "           Kannada   Tamil  Telugu  Malayalam\n",
      "Kannada     100.00   44.84   15.97        0.0\n",
      "Tamil        35.58  100.00   28.87        0.0\n",
      "Telugu       40.88   15.97  100.00        0.0\n",
      "Malayalam    49.28   15.97    0.00      100.0\n",
      "\n",
      "Similarity Scores Matrix:\n",
      "           Kannada   Tamil  Telugu  Malayalam\n",
      "Kannada     1.0000  0.6484  0.5947     0.4021\n",
      "Tamil       0.6787  1.0000  0.6695     0.3574\n",
      "Telugu      0.6311  0.7788  1.0000     0.7056\n",
      "Malayalam   0.6930  0.6731  0.5437     1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your trained model/tokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = \"Sujal-mT5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Sentence similarity model\n",
    "sim_model = SentenceTransformer(\"l3cube-pune/indic-sentence-similarity-sbert\").to(device)\n",
    "\n",
    "# BLEU smoother\n",
    "smoother = SmoothingFunction().method2\n",
    "\n",
    "# Languages and codes\n",
    "langs = ['Kannada', 'Tamil', 'Telugu', 'Malayalam']\n",
    "lang_code_map = {\n",
    "    'Kannada': 'Kannada',\n",
    "    'Tamil': 'Tamil',\n",
    "    'Telugu': 'Telugu',\n",
    "    'Malayalam': 'Malayalam'\n",
    "}\n",
    "\n",
    "# Dummy dataset (replace with your real dataset)\n",
    "# Ensure df is already loaded and has columns for each language\n",
    "# Example:\n",
    "# df = pd.read_csv(\"your_multilingual_data.csv\") or defined manually\n",
    "df = pd.DataFrame({\n",
    "    'Kannada': ['ನಾನು ಶಾಲೆಗೆ ಹೋಗುತ್ತಿದ್ದೇನೆ', 'ಅವನ ಹೆಸರು ರಾಮ'],\n",
    "    'Tamil': ['நான் பள்ளிக்கு செல்கிறேன்', 'அவன் பெயர் ராம்'],\n",
    "    'Telugu': ['నేను పాఠశాలకు వెళుతున్నాను', 'అతని పేరు రామ్'],\n",
    "    'Malayalam': ['ഞാൻ സ്കൂളിലേക്ക് പോകുന്നു', 'അവന്റെ പേര് രാമൻ']\n",
    "})\n",
    "\n",
    "# Translation function\n",
    "def translate_batch(texts, src_lang, tgt_lang):\n",
    "    prefix = f\"translate {lang_code_map[src_lang]} to {lang_code_map[tgt_lang]}: \"\n",
    "    inputs = tokenizer([prefix + text for text in texts], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# BLEU score\n",
    "def sentence_bleu_score(reference, candidate):\n",
    "    return sentence_bleu(\n",
    "        [reference.split()],\n",
    "        candidate.split(),\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),\n",
    "        smoothing_function=smoother\n",
    "    )\n",
    "\n",
    "# Cosine similarity\n",
    "def sentence_similarity(ref, cand):\n",
    "    emb_ref = sim_model.encode(ref, convert_to_tensor=True)\n",
    "    emb_cand = sim_model.encode(cand, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(emb_ref, emb_cand).item()\n",
    "\n",
    "# Initialize score matrices\n",
    "bleu_df = pd.DataFrame(index=langs, columns=langs, dtype=float)\n",
    "sim_df = pd.DataFrame(index=langs, columns=langs, dtype=float)\n",
    "\n",
    "# Evaluation\n",
    "for src_lang in langs:\n",
    "    src_texts = df[src_lang].tolist()\n",
    "    for tgt_lang in langs:\n",
    "        tgt_texts = df[tgt_lang].tolist()\n",
    "\n",
    "        if src_lang == tgt_lang:\n",
    "            bleu_df.loc[src_lang, tgt_lang] = 100.0\n",
    "            sim_df.loc[src_lang, tgt_lang] = 1.0\n",
    "            print(f\"Skipping {src_lang} -> {tgt_lang} (same language), BLEU=100, Similarity=1.0\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTranslating {src_lang} -> {tgt_lang} ...\")\n",
    "        translations = translate_batch(src_texts, src_lang, tgt_lang)\n",
    "\n",
    "        bleu_scores = []\n",
    "        sim_scores = []\n",
    "\n",
    "        for i, (ref, pred) in enumerate(tqdm(zip(tgt_texts, translations), total=len(tgt_texts), desc=f\"{src_lang}->{tgt_lang}\")):\n",
    "            try:\n",
    "                bleu_val = sentence_bleu_score(ref, pred)\n",
    "            except Exception:\n",
    "                bleu_val = 0.0\n",
    "            sim_val = sentence_similarity(ref, pred)\n",
    "\n",
    "            bleu_scores.append(bleu_val)\n",
    "            sim_scores.append(sim_val)\n",
    "\n",
    "        avg_bleu = np.mean(bleu_scores) * 100\n",
    "        avg_sim = np.mean(sim_scores)\n",
    "\n",
    "        bleu_df.loc[src_lang, tgt_lang] = avg_bleu\n",
    "        sim_df.loc[src_lang, tgt_lang] = avg_sim\n",
    "\n",
    "# Output matrices\n",
    "print(\"\\nBLEU Scores Matrix (%):\")\n",
    "print(bleu_df.round(2))\n",
    "\n",
    "print(\"\\nSimilarity Scores Matrix:\")\n",
    "print(sim_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Kannada -> Kannada (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Kannada -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Tamil: 100%|██████████| 50/50 [00:00<00:00, 50.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Kannada -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Telugu: 100%|██████████| 50/50 [00:00<00:00, 51.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Kannada -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kannada->Malayalam: 100%|██████████| 50/50 [00:00<00:00, 50.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Tamil -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Kannada: 100%|██████████| 50/50 [00:00<00:00, 51.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Tamil -> Tamil (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Tamil -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Telugu: 100%|██████████| 50/50 [00:01<00:00, 49.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Tamil -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tamil->Malayalam: 100%|██████████| 50/50 [00:00<00:00, 52.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Telugu -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Kannada: 100%|██████████| 50/50 [00:00<00:00, 50.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Telugu -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Tamil: 100%|██████████| 50/50 [00:01<00:00, 46.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Telugu -> Telugu (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "Translating Telugu -> Malayalam ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Telugu->Malayalam: 100%|██████████| 50/50 [00:00<00:00, 50.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Malayalam -> Kannada ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Kannada: 100%|██████████| 50/50 [00:00<00:00, 51.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Malayalam -> Tamil ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Tamil: 100%|██████████| 50/50 [00:01<00:00, 47.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translating Malayalam -> Telugu ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Malayalam->Telugu: 100%|██████████| 50/50 [00:00<00:00, 50.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Malayalam -> Malayalam (same language), BLEU=100, Similarity=1.0\n",
      "\n",
      "BLEU Scores Matrix (%):\n",
      "           Kannada   Tamil  Telugu  Malayalam\n",
      "Kannada     100.00    2.21    1.54       1.37\n",
      "Tamil         1.84  100.00    1.46       0.63\n",
      "Telugu        1.45    1.21  100.00       0.51\n",
      "Malayalam     1.71    1.17    1.42     100.00\n",
      "\n",
      "Similarity Scores Matrix:\n",
      "           Kannada   Tamil  Telugu  Malayalam\n",
      "Kannada     1.0000  0.7304  0.7082     0.7265\n",
      "Tamil       0.7796  1.0000  0.6925     0.7120\n",
      "Telugu      0.7636  0.7276  1.0000     0.7320\n",
      "Malayalam   0.7640  0.7243  0.7323     1.0000\n",
      "\n",
      "Average BLEU Score (excluding same-language): 1.38%\n",
      "Average Cosine Similarity Score (excluding same-language): 0.7327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load your trained model/tokenizer\n",
    "model_path = \"Sujal-mT5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load sentence similarity model\n",
    "sim_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# BLEU smoother\n",
    "smoother = SmoothingFunction().method4\n",
    "\n",
    "langs = ['Kannada', 'Tamil', 'Telugu', 'Malayalam']\n",
    "lang_code_map = {\n",
    "    'Kannada': 'Kannada',\n",
    "    'Tamil': 'Tamil',\n",
    "    'Telugu': 'Telugu',\n",
    "    'Malayalam': 'Malayalam'\n",
    "}\n",
    "\n",
    "# Load your parallel dataframe (replace with actual path)\n",
    "df = pd.read_csv(\"final_dataset.csv\") \n",
    "df=df.head(50) # Replace with your CSV file path\n",
    "\n",
    "def translate_batch(texts, src_lang, tgt_lang):\n",
    "    prefix = f\"translate {lang_code_map[src_lang]} to {lang_code_map[tgt_lang]}: \"\n",
    "    inputs = tokenizer([prefix + text for text in texts], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "def sentence_bleu_score(reference, candidate):\n",
    "    return sentence_bleu(\n",
    "        [reference.split()],\n",
    "        candidate.split(),\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),\n",
    "        smoothing_function=smoother\n",
    "    )\n",
    "\n",
    "def sentence_similarity(ref, cand):\n",
    "    emb_ref = sim_model.encode(ref, convert_to_tensor=True)\n",
    "    emb_cand = sim_model.encode(cand, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(emb_ref, emb_cand).item()\n",
    "\n",
    "# Matrices for results\n",
    "bleu_df = pd.DataFrame(index=langs, columns=langs, dtype=float)\n",
    "sim_df = pd.DataFrame(index=langs, columns=langs, dtype=float)\n",
    "\n",
    "# Compute BLEU and similarity\n",
    "for src_lang in langs:\n",
    "    src_texts = df[src_lang].tolist()\n",
    "    for tgt_lang in langs:\n",
    "        tgt_texts = df[tgt_lang].tolist()\n",
    "        if src_lang == tgt_lang:\n",
    "            bleu_df.loc[src_lang, tgt_lang] = 100.0\n",
    "            sim_df.loc[src_lang, tgt_lang] = 1.0\n",
    "            print(f\"Skipping {src_lang} -> {tgt_lang} (same language), BLEU=100, Similarity=1.0\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTranslating {src_lang} -> {tgt_lang} ...\")\n",
    "        translations = translate_batch(src_texts, src_lang, tgt_lang)\n",
    "\n",
    "        bleu_scores = []\n",
    "        sim_scores = []\n",
    "        for i, (ref, pred) in enumerate(tqdm(zip(tgt_texts, translations), total=len(tgt_texts), desc=f\"{src_lang}->{tgt_lang}\")):\n",
    "            try:\n",
    "                bleu_val = sentence_bleu_score(ref, pred)\n",
    "            except Exception:\n",
    "                bleu_val = 0.0\n",
    "            sim_val = sentence_similarity(ref, pred)\n",
    "            bleu_scores.append(bleu_val)\n",
    "            sim_scores.append(sim_val)\n",
    "\n",
    "        avg_bleu = np.mean(bleu_scores) * 100\n",
    "        avg_sim = np.mean(sim_scores)\n",
    "\n",
    "        bleu_df.loc[src_lang, tgt_lang] = avg_bleu\n",
    "        sim_df.loc[src_lang, tgt_lang] = avg_sim\n",
    "\n",
    "print(\"\\nBLEU Scores Matrix (%):\")\n",
    "print(bleu_df.round(2))\n",
    "\n",
    "print(\"\\nSimilarity Scores Matrix:\")\n",
    "print(sim_df.round(4))\n",
    "\n",
    "# ==== Calculate Averages (excluding same-language) ====\n",
    "mask = ~np.eye(len(langs), dtype=bool)\n",
    "bleu_array = bleu_df.values.astype(float)\n",
    "sim_array = sim_df.values.astype(float)\n",
    "\n",
    "avg_bleu_score = bleu_array[mask].mean()\n",
    "avg_sim_score = sim_array[mask].mean()\n",
    "\n",
    "print(f\"\\nAverage BLEU Score (excluding same-language): {avg_bleu_score:.2f}%\")\n",
    "print(f\"Average Cosine Similarity Score (excluding same-language): {avg_sim_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 11:14:26.619437: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-15 11:14:26.945063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747307667.073740    1899 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747307667.108781    1899 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747307667.385536    1899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747307667.385708    1899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747307667.385711    1899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747307667.385713    1899 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-15 11:14:27.417841: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Kannada -> Kannada (same language), chrF++ = 100.0\n",
      "\n",
      "Translating Kannada -> Tamil ...\n",
      "\n",
      "Translating Kannada -> Telugu ...\n",
      "\n",
      "Translating Kannada -> Malayalam ...\n",
      "\n",
      "Translating Tamil -> Kannada ...\n",
      "Skipping Tamil -> Tamil (same language), chrF++ = 100.0\n",
      "\n",
      "Translating Tamil -> Telugu ...\n",
      "\n",
      "Translating Tamil -> Malayalam ...\n",
      "\n",
      "Translating Telugu -> Kannada ...\n",
      "\n",
      "Translating Telugu -> Tamil ...\n",
      "Skipping Telugu -> Telugu (same language), chrF++ = 100.0\n",
      "\n",
      "Translating Telugu -> Malayalam ...\n",
      "\n",
      "Translating Malayalam -> Kannada ...\n",
      "\n",
      "Translating Malayalam -> Tamil ...\n",
      "\n",
      "Translating Malayalam -> Telugu ...\n",
      "Skipping Malayalam -> Malayalam (same language), chrF++ = 100.0\n",
      "\n",
      "chrF++ Score Matrix (%):\n",
      "           Kannada   Tamil  Telugu  Malayalam\n",
      "Kannada     100.00   76.99   17.67      13.41\n",
      "Tamil        45.12  100.00   13.13      21.68\n",
      "Telugu       67.08   34.54  100.00      16.50\n",
      "Malayalam    69.09   34.99   20.10     100.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = \"Sujal-mT5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Dravidian language mapping\n",
    "langs = ['Kannada', 'Tamil', 'Telugu', 'Malayalam']\n",
    "lang_code_map = {\n",
    "    'Kannada': 'Kannada',\n",
    "    'Tamil': 'Tamil',\n",
    "    'Telugu': 'Telugu',\n",
    "    'Malayalam': 'Malayalam'\n",
    "}\n",
    "\n",
    "# 10-example dataset (you can replace this with your real one)\n",
    "df = pd.DataFrame({\n",
    "    'Kannada': [\n",
    "        'ನಾನು ಶಾಲೆಗೆ ಹೋಗುತ್ತಿದ್ದೇನೆ', 'ಅವನ ಹೆಸರು ರಾಮ', 'ಅವಳು ಪುಸ್ತಕ ಓದುತ್ತಿದ್ದಾಳೆ',\n",
    "        'ನಾವು ಮನೆಗೆ ಹೋಗುತ್ತೇವೆ', 'ಅವರು ಚೆನ್ನಾಗಿದ್ದಾರೆ', 'ನೀವು ಏನು ಮಾಡುತ್ತಿದ್ದೀರಿ?',\n",
    "        'ಅವರು ಆಟವಾಡುತ್ತಿದ್ದಾರೆ', 'ಅದು ನನ್ನ ಕಾರು', 'ನಾನು ಭತ್ತ ತಿನ್ನುತ್ತಿದ್ದೇನೆ', 'ಅವನು ಸಿನಿಮಾ ನೋಡುತ್ತಿದ್ದಾನೆ'\n",
    "    ],\n",
    "    'Tamil': [\n",
    "        'நான் பள்ளிக்கு செல்கிறேன்', 'அவன் பெயர் ராம்', 'அவள் புத்தகம் படிக்கிறாள்',\n",
    "        'நாம் வீட்டுக்கு செல்கிறோம்', 'அவர்கள் நலமாக இருக்கிறார்கள்', 'நீங்கள் என்ன செய்து கொண்டிருக்கிறீர்கள்?',\n",
    "        'அவர்கள் விளையாடுகிறார்கள்', 'அது என் கார்', 'நான் அரிசி சாப்பிடுகிறேன்', 'அவன் படம் பார்க்கிறான்'\n",
    "    ],\n",
    "    'Telugu': [\n",
    "        'నేను పాఠశాలకు వెళుతున్నాను', 'అతని పేరు రామ్', 'ఆమె పుస్తకం చదువుతుంది',\n",
    "        'మేము ఇంటికి వెళ్తున్నాము', 'వారు బాగున్నారు', 'మీరు ఏమి చేస్తున్నారు?',\n",
    "        'వారు ఆడుతున్నారు', 'అది నా కారు', 'నేను అన్నం తింటున్నాను', 'అతను సినిమా చూస్తున్నాడు'\n",
    "    ],\n",
    "    'Malayalam': [\n",
    "        'ഞാൻ സ്കൂളിലേക്ക് പോകുന്നു', 'അവന്റെ പേര് രാമൻ', 'അവൾ പുസ്തകം വായിക്കുന്നു',\n",
    "        'ഞങ്ങൾ വീട്ടിലേക്ക് പോകുന്നു', 'അവർ സുഖമായിരിക്കുന്നു', 'നിങ്ങൾ എന്താണ് ചെയ്യുന്നത്?',\n",
    "        'അവർ കളിക്കുന്നു', 'അത് എന്റെ കാർ ആണ്', 'ഞാൻ അരി കഴിക്കുന്നു', 'അവൻ സിനിമ കാണുന്നു'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Translation function\n",
    "def translate_batch(texts, src_lang, tgt_lang):\n",
    "    prefix = f\"translate {lang_code_map[src_lang]} to {lang_code_map[tgt_lang]}: \"\n",
    "    inputs = tokenizer([prefix + text for text in texts], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# chrF++ Score Calculation Function\n",
    "def compute_chrfpp(refs, preds):\n",
    "    refs = [[ref] for ref in refs]  # sacrebleu expects list of references per sentence\n",
    "    chrf_score = sacrebleu.corpus_chrf(preds, refs, word_order=2)\n",
    "    return chrf_score.score\n",
    "\n",
    "# Score matrix\n",
    "chrf_df = pd.DataFrame(index=langs, columns=langs, dtype=float)\n",
    "\n",
    "# Evaluate translation directions\n",
    "for src_lang in langs:\n",
    "    src_texts = df[src_lang].tolist()\n",
    "    for tgt_lang in langs:\n",
    "        tgt_texts = df[tgt_lang].tolist()\n",
    "\n",
    "        if src_lang == tgt_lang:\n",
    "            chrf_df.loc[src_lang, tgt_lang] = 100.0\n",
    "            print(f\"Skipping {src_lang} -> {tgt_lang} (same language), chrF++ = 100.0\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTranslating {src_lang} -> {tgt_lang} ...\")\n",
    "        translations = translate_batch(src_texts, src_lang, tgt_lang)\n",
    "\n",
    "        # Compute chrF++\n",
    "        chrf_score = compute_chrfpp(tgt_texts, translations)\n",
    "        chrf_df.loc[src_lang, tgt_lang] = chrf_score\n",
    "\n",
    "# Output results\n",
    "print(\"\\nchrF++ Score Matrix (%):\")\n",
    "print(chrf_df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Kannada -> Kannada (same language), chrF++ = 100.0\n",
      "\n",
      "Translating Kannada -> Tamil ...\n",
      "\n",
      "Translating Kannada -> Telugu ...\n",
      "\n",
      "Translating Kannada -> Malayalam ...\n",
      "\n",
      "Translating Tamil -> Kannada ...\n",
      "Skipping Tamil -> Tamil (same language), chrF++ = 100.0\n",
      "\n",
      "Translating Tamil -> Telugu ...\n",
      "\n",
      "Translating Tamil -> Malayalam ...\n",
      "\n",
      "Translating Telugu -> Kannada ...\n",
      "\n",
      "Translating Telugu -> Tamil ...\n",
      "Skipping Telugu -> Telugu (same language), chrF++ = 100.0\n",
      "\n",
      "Translating Telugu -> Malayalam ...\n",
      "\n",
      "Translating Malayalam -> Kannada ...\n",
      "\n",
      "Translating Malayalam -> Tamil ...\n",
      "\n",
      "Translating Malayalam -> Telugu ...\n",
      "Skipping Malayalam -> Malayalam (same language), chrF++ = 100.0\n",
      "\n",
      "chrF++ Score Matrix (%):\n",
      "           Kannada   Tamil  Telugu  Malayalam\n",
      "Kannada     100.00   15.53   15.05      11.29\n",
      "Tamil        40.43  100.00    9.39       7.47\n",
      "Telugu       11.65    9.40  100.00      11.66\n",
      "Malayalam    12.05    7.95    5.32     100.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = \"Sujal-mT5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Language list and mapping\n",
    "langs = ['Kannada', 'Tamil', 'Telugu', 'Malayalam']\n",
    "lang_code_map = {\n",
    "    'Kannada': 'Kannada',\n",
    "    'Tamil': 'Tamil',\n",
    "    'Telugu': 'Telugu',\n",
    "    'Malayalam': 'Malayalam'\n",
    "}\n",
    "\n",
    "# Load your CSV dataset (make sure it has the columns: Kannada, Tamil, Telugu, Malayalam)\n",
    "df = pd.read_csv(\"final_dataset.csv\")\n",
    "df=df.head(50)\n",
    "\n",
    "# Limit to 10 rows to compute chrF++ (as requested)\n",
    "df = df[langs].dropna().head(10)\n",
    "\n",
    "# Translation function\n",
    "def translate_batch(texts, src_lang, tgt_lang):\n",
    "    prefix = f\"translate {lang_code_map[src_lang]} to {lang_code_map[tgt_lang]}: \"\n",
    "    inputs = tokenizer([prefix + text for text in texts], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# chrF++ scoring\n",
    "def compute_chrfpp(refs, preds):\n",
    "    refs = [[ref] for ref in refs]  # sacrebleu expects list of references per sentence\n",
    "    chrf_score = sacrebleu.corpus_chrf(preds, refs, word_order=2)\n",
    "    return chrf_score.score\n",
    "\n",
    "# chrF++ score matrix\n",
    "chrf_df = pd.DataFrame(index=langs, columns=langs, dtype=float)\n",
    "\n",
    "# Compute scores\n",
    "for src_lang in langs:\n",
    "    src_texts = df[src_lang].tolist()\n",
    "    for tgt_lang in langs:\n",
    "        tgt_texts = df[tgt_lang].tolist()\n",
    "\n",
    "        if src_lang == tgt_lang:\n",
    "            chrf_df.loc[src_lang, tgt_lang] = 100.0\n",
    "            print(f\"Skipping {src_lang} -> {tgt_lang} (same language), chrF++ = 100.0\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTranslating {src_lang} -> {tgt_lang} ...\")\n",
    "        translations = translate_batch(src_texts, src_lang, tgt_lang)\n",
    "        chrf_score = compute_chrfpp(tgt_texts, translations)\n",
    "        chrf_df.loc[src_lang, tgt_lang] = chrf_score\n",
    "\n",
    "# Print results\n",
    "print(\"\\nchrF++ Score Matrix (%):\")\n",
    "print(chrf_df.round(2))\n",
    "\n",
    "# Optionally save\n",
    "chrf_df.to_csv(\"chrf_scores_matrix.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
